{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":992580,"sourceType":"datasetVersion","datasetId":543939}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import DenseNet201\nfrom tensorflow.keras.applications.densenet import preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers, models\nimport os\nimport pandas as pd\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:33:13.770411Z","iopub.execute_input":"2024-12-18T13:33:13.771238Z","iopub.status.idle":"2024-12-18T13:33:13.775938Z","shell.execute_reply.started":"2024-12-18T13:33:13.771201Z","shell.execute_reply":"2024-12-18T13:33:13.775025Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"Dataset = '/kaggle/input/pins-face-recognition/105_classes_pins_dataset'\n\n# Load all image paths and labels\nall_image_paths = []\nall_labels = []\nfor root, _, files in os.walk(Dataset):\n    for file in files:\n        if file.endswith(('.jpg', '.jpeg', '.png')):\n            label = os.path.basename(root)\n            all_image_paths.append(os.path.join(root, file))\n            all_labels.append(label)\n\n# Count the number of images per label\nlabel_counts = Counter(all_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:33:13.780580Z","iopub.execute_input":"2024-12-18T13:33:13.780807Z","iopub.status.idle":"2024-12-18T13:33:28.995540Z","shell.execute_reply.started":"2024-12-18T13:33:13.780785Z","shell.execute_reply":"2024-12-18T13:33:28.994774Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Simple random split without stratification\ntrain_paths, test_paths, train_labels, test_labels = train_test_split(\n    all_image_paths, all_labels, test_size=0.2, random_state=42\n)\nprint(f\"Number of Classes : {len(label_counts)}\")\nprint(f\"Number of training samples: {len(train_paths)}\")\nprint(f\"Number of testing samples: {len(test_paths)}\")\n\n# Define image data generators\ntrain_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # Preprocess for DenseNet\n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    zoom_range=0.1,\n    horizontal_flip=True\n)\n\ntest_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input  # Preprocess for DenseNet\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:33:28.996855Z","iopub.execute_input":"2024-12-18T13:33:28.997140Z","iopub.status.idle":"2024-12-18T13:33:29.012361Z","shell.execute_reply.started":"2024-12-18T13:33:28.997113Z","shell.execute_reply":"2024-12-18T13:33:29.011408Z"}},"outputs":[{"name":"stdout","text":"Number of Classes : 105\nNumber of training samples: 14027\nNumber of testing samples: 3507\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"train_generator = train_datagen.flow_from_dataframe(\n    dataframe=pd.DataFrame({'filename': train_paths, 'class': train_labels}),\n    x_col='filename',\n    y_col='class',\n    target_size=(224, 224),  # DenseNet input size\n    batch_size=64,\n    class_mode='categorical'\n)\n\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe=pd.DataFrame({'filename': test_paths, 'class': test_labels}),\n    x_col='filename',\n    y_col='class',\n    target_size=(224, 224),  # DenseNet input size\n    batch_size=64,\n    class_mode='categorical'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:33:29.013675Z","iopub.execute_input":"2024-12-18T13:33:29.014470Z","iopub.status.idle":"2024-12-18T13:33:31.023204Z","shell.execute_reply.started":"2024-12-18T13:33:29.014419Z","shell.execute_reply":"2024-12-18T13:33:31.022302Z"}},"outputs":[{"name":"stdout","text":"Found 14027 validated image filenames belonging to 105 classes.\nFound 3507 validated image filenames belonging to 105 classes.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_model = DenseNet201(\n    weights='imagenet', include_top=False, input_shape=(224, 224, 3)\n)\nprint(f\"Number of layers in the model: {len(base_model.layers)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:33:31.024885Z","iopub.execute_input":"2024-12-18T13:33:31.025177Z","iopub.status.idle":"2024-12-18T13:33:33.746053Z","shell.execute_reply.started":"2024-12-18T13:33:31.025150Z","shell.execute_reply":"2024-12-18T13:33:33.745205Z"}},"outputs":[{"name":"stdout","text":"Number of layers in the model: 707\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"strategy = tf.distribute.MirroredStrategy()\nprint(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n\n# Open a strategy scope\nwith strategy.scope():\n    # Load DenseNet201 model without top layers\n    base_model = DenseNet201(\n        weights='imagenet', include_top=False, input_shape=(224, 224, 3)\n    )\n    \n    # Freeze the base model\n    base_model.trainable = False\n    \n    # Add custom layers on top of DenseNet\n    model = tf.keras.Sequential([\n        tf.keras.Input(shape=(224, 224, 3)),  # Input layer\n        base_model,  # DenseNet base model\n        layers.GlobalAveragePooling2D(),  # Global pooling layer\n        layers.Dense(512, activation='relu'),  # Custom dense layer\n        layers.Dropout(0.5),  # Dropout to reduce overfitting\n        layers.Dense(len(train_generator.class_indices), activation='softmax')  # Output layer\n    ])\n    \n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\nmodel.summary()\nprint(f\"Number of layers in the model: {len(model.layers)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:33:33.747390Z","iopub.execute_input":"2024-12-18T13:33:33.748035Z","iopub.status.idle":"2024-12-18T13:33:36.868653Z","shell.execute_reply.started":"2024-12-18T13:33:33.747973Z","shell.execute_reply":"2024-12-18T13:33:36.867830Z"}},"outputs":[{"name":"stdout","text":"Number of devices: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ densenet201 (\u001b[38;5;33mFunctional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1920\u001b[0m)     │    \u001b[38;5;34m18,321,984\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1920\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m983,552\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m105\u001b[0m)            │        \u001b[38;5;34m53,865\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ densenet201 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1920</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">18,321,984</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1920</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">983,552</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">105</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">53,865</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,359,401\u001b[0m (73.85 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,359,401</span> (73.85 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,037,417\u001b[0m (3.96 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,037,417</span> (3.96 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m18,321,984\u001b[0m (69.89 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,321,984</span> (69.89 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Number of layers in the model: 5\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"with strategy.scope():\n    history = model.fit(\n        train_generator,\n        validation_data=test_generator,\n        epochs=5\n    )\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:33:36.869801Z","iopub.execute_input":"2024-12-18T13:33:36.870161Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m 39/220\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:19\u001b[0m 771ms/step - accuracy: 0.0128 - loss: 4.8709","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nwith strategy.scope():\n    base_model.trainable = True\n\n    for layer in base_model.layers[:100]:\n        layer.trainable = False\n    \n    # Recompile the model after unfreezing layers (important step)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n\n    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n    history = model.fit(\n        train_generator,\n        validation_data=test_generator,\n        epochs=27, # Max number of epochs\n        callbacks=[early_stopping] # Include the callback here\n    )\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ntrain_acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\n# Create the plot\nplt.figure(figsize=(10, 6))\nplt.plot(train_acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\n\n# Add labels and title\nplt.title('Training and Validation Accuracy Over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Show the plot\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the model on the test set\ntest_loss, test_accuracy = model.evaluate(test_generator)\n\n# Print the evaluation results\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test Accuracy: {test_accuracy}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport numpy as np\nimg_path = '/kaggle/input/pins-face-recognition/105_classes_pins_dataset/pins_amber heard/amber heard219_327.jpg'\n\nimg = load_img(img_path, target_size=(224, 224))  \nimg_array = img_to_array(img) / 255.0  \nimg_array = np.expand_dims(img_array, axis=0)  \n\n# Get the mapping of class indices to class names\nclass_indices = train_generator.class_indices  # Assuming the train_generator was used in training\nclass_names = {v: k for k, v in class_indices.items()}  # Reverse the dictionary\n\n# Predict the class\npredictions = model.predict(img_array)\npredicted_class_index = np.argmax(predictions, axis=1)[0]\npredicted_class_name = class_names[predicted_class_index]\n\n\n# Print the predicted class number and name\nprint(f\"Predicted Class Index: {predicted_class_index}\")\nprint(f\"Predicted Class Name: {predicted_class_name}\")\n\n# Display the image with the predicted class name\nplt.imshow(img)\nplt.title(f\"Predicted: {predicted_class_name}\")\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\n\n# Generate predictions for the test dataset\ny_pred_probs = model.predict(test_generator)\ny_pred_classes = np.argmax(y_pred_probs, axis=1)  # Convert probabilities to class indices\ny_true_classes = test_generator.classes            # True labels\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n\n# Plot Confusion Matrix\nplt.figure(figsize=(12, 8))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=test_generator.class_indices.keys(),\n            yticklabels=test_generator.class_indices.keys())\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Class\")\nplt.ylabel(\"True Class\")\nplt.show()\n\n# ROC Curve\n# One-hot encode the true labels\ny_true_binarized = label_binarize(y_true_classes, classes=list(range(len(test_generator.class_indices))))\nn_classes = y_true_binarized.shape[1]\n\n# Calculate ROC curve and AUC for each class\nfpr = {}\ntpr = {}\nroc_auc = {}\n\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_true_binarized[:, i], y_pred_probs[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Plot ROC curve for each class\nplt.figure(figsize=(10, 8))\nfor i in range(n_classes):\n    plt.plot(fpr[i], tpr[i], label=f\"Class {i} (AUC = {roc_auc[i]:.2f})\")\n\nplt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for Multi-Class Classification\")\nplt.legend(loc=\"lower right\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}